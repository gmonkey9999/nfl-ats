#!/usr/bin/env python3
"""
predict_ats_outcomes.py

Batch infer ATS outcomes (away_cover / push / home_cover) using a trained XGBoost
model produced by train_xgb_cover.py.

Requirements
------------
This script expects the training job to have saved **both**:
- xgb_model.json               (the trained booster)
- feature_order.json           (ordered list of final feature names after preprocessing)
- preproc_meta.json            (lists of numeric and categorical base columns)

These files are created by the updated `train_xgb_cover.py` (already updated in your canvas).

Inputs
------
--in_csv        : CSV of upcoming games with the same base columns the model used
--model         : Path to xgb_model.json
--feature_order : Path to feature_order.json (defaults next to model)
--preproc_meta  : Path to preproc_meta.json (defaults next to model)
--out_csv       : Output CSV for predictions

Notes
-----
- Categorical expansion is rebuilt from `feature_order.json` by matching patterns like
  "home_team_KC" (column = home_team, value = KC). Any missing category columns are
  added with zeros. Unknown categories in input are ignored (all-zeros across that
  column's expected one-hots).
- Numeric columns missing in input are filled with 0.0 by default.
"""
from __future__ import annotations
import argparse
import json
import logging
from pathlib import Path
from typing import List, Tuple

import numpy as np
import pandas as pd
from xgboost import XGBClassifier

logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
)

LABELS = ["away_cover", "push", "home_cover"]


def load_meta(model_path: Path, feature_order: str | None, preproc_meta: str | None) -> tuple[list[str], dict]:
    model_dir = model_path.parent
    fo_path = Path(feature_order) if feature_order else (model_dir / 'feature_order.json')
    pm_path = Path(preproc_meta) if preproc_meta else (model_dir / 'preproc_meta.json')

    if not fo_path.exists():
        raise SystemExit(
            f"feature_order.json not found at {fo_path}. Re-train with the updated train_xgb_cover.py to export it."
        )
    if not pm_path.exists():
        raise SystemExit(
            f"preproc_meta.json not found at {pm_path}. Re-train with the updated train_xgb_cover.py to export it."
        )

    feature_order_list = json.loads(fo_path.read_text())
    pre_meta = json.loads(pm_path.read_text())
    return feature_order_list, pre_meta


def build_design_matrix(df: pd.DataFrame, feature_order: List[str], pre_meta: dict) -> np.ndarray:
    """Reconstruct the exact training-time feature matrix ordering.

    - Numeric columns: copied directly (filled with 0 if missing).
    - Categorical one-hots: generated by parsing feature names like 'col_val'.
    """
    num_cols = set(pre_meta.get('num_cols', []))
    cat_cols = set(pre_meta.get('cat_cols', []))

    X = np.zeros((len(df), len(feature_order)), dtype=float)

    # Pre-normalize categoricals for safe comparisons
    for c in cat_cols:
        if c in df.columns:
            df[c] = df[c].astype(str).str.upper().str.strip()

    for j, fname in enumerate(feature_order):
        if fname in df.columns and fname not in cat_cols:
            # Direct numeric passthrough
            X[:, j] = pd.to_numeric(df[fname], errors='coerce').fillna(0.0).to_numpy()
            continue

        # Try to decode one-hot as "<col>_<value>"
        assigned = False
        for col in cat_cols:
            prefix = f"{col}_"
            if fname.startswith(prefix):
                value = fname[len(prefix):]
                if col in df.columns:
                    X[:, j] = (df[col].astype(str).str.upper().str.strip() == value).astype(float).to_numpy()
                else:
                    X[:, j] = 0.0
                assigned = True
                break
        if assigned:
            continue

        # Fallback: treat as numeric column that may be missing in input
        if fname in df.columns:
            X[:, j] = pd.to_numeric(df[fname], errors='coerce').fillna(0.0).to_numpy()
        else:
            X[:, j] = 0.0

    return X


def main():
    ap = argparse.ArgumentParser(description='Predict ATS outcome probabilities using a trained XGBoost model')
    ap.add_argument('--in_csv', required=True, help='CSV with upcoming games')
    ap.add_argument('--model', required=True, help='Path to xgb_model.json')
    ap.add_argument('--feature_order', default=None, help='Path to feature_order.json (optional)')
    ap.add_argument('--preproc_meta', default=None, help='Path to preproc_meta.json (optional)')
    ap.add_argument('--out_csv', default='predictions.csv', help='Output CSV path')
    args = ap.parse_args()

    model_path = Path(args.model)

    # Load input
    df = pd.read_csv(args.in_csv)

    # Load metadata
    feature_order_list, pre_meta = load_meta(model_path, args.feature_order, args.preproc_meta)

    # Build design matrix
    X = build_design_matrix(df.copy(), feature_order_list, pre_meta)

    # Load model
    clf = XGBClassifier()
    clf.load_model(str(model_path))

    # Predict
    proba = clf.predict_proba(X)
    pred_idx = proba.argmax(axis=1)
    pred_label = [LABELS[i] for i in pred_idx]

    # Stitch output
    out = df.copy()
    out['p_away_cover'] = proba[:, 0]
    out['p_push'] = proba[:, 1]
    out['p_home_cover'] = proba[:, 2]
    out['pred_label'] = pred_label

    Path(args.out_csv).parent.mkdir(parents=True, exist_ok=True)
    out.to_csv(args.out_csv, index=False)
    logging.info(f"Saved predictions â†’ {args.out_csv}")


if __name__ == '__main__':
    main()
